\documentclass[10pt,journal,compsoc]{IEEEtran}
\usepackage{alltt}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{color}
\usepackage[labelfont=bf,textfont={bf}]{caption}
\usepackage{subcaption}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\usepackage{boxedminipage}
\usepackage{notoccite}
\usepackage{array}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\usepackage{enumerate}
\usepackage{booktabs}
% \usepackage{minted}
\usepackage[framemethod=tikz]{mdframed}
\usepackage{lipsum}
\usetikzlibrary{shadows}
\usepackage{mathtools}
 \usepackage{paralist}
 \definecolor{light-gray}{gray}{0.80}
\usepackage{graphics}
\usepackage[shortlabels]{enumitem}
%\usepackage{balance}
\usepackage{dblfloatfix}

%\usepackage{hyperref}
%\usepackage{cleveref}
% \usepackage{colortbl}
\newmdenv[
  tikzsetting= {fill=light-gray},
  linewidth=1pt,
  roundcorner=0pt, 
  shadow=false
]{myshadowbox}
\usepackage{colortbl} 
% \usepackage{subcaption}
\usepackage{blindtext, graphicx}
\usepackage{textcomp}
% \usepackage{mathtools}
\usepackage[hidelinks]{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{caption}
\usepackage{color}
\usepackage[final]{listings}
% \usepackage{graphicx} 
% \usepackage{multirow}
\usepackage{balance}
\usepackage{picture}
\usepackage{relsize}
\usepackage{multicol}
\usepackage{soul}
\usepackage{enumitem}
\setitemize{noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt,leftmargin=*}
\setenumerate{noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt,leftmargin=*}
\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
% \usepackage{array}
\usepackage{makecell}
\usepackage{bigstrut}
\newcommand{\tion}[1]{\S\ref{sect:#1}}
\newcommand{\fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\tab}[1]{Table~\ref{tab:#1}}

\newcommand{\tbl}[1]{Table~\ref{tbl:#1}}
\definecolor{comment_color}{rgb}{0.5, 0, 1}
\definecolor{steel}{rgb}{0.1, 0.3, 0.5} 
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\todoc}[2]{{\textcolor{#1}{\textbf{[[#2]]}}}}
\newcommand{\todobrown}[1]{\todoc{green}{\textbf{[[#1]]}}}
\newcommand{\todoblue}[1]{\todoc{blue}{\textbf{[[#1]]}}}
\newcommand{\todoorange}[1]{\todoc{cyan}{\textbf{[[#1]]}}}
\newcommand{\todored}[1]{\todoc{red}{\textbf{[[#1]]}}}
\newcommand{\rahul}[1]{{\textcolor{steel}{#1}}}
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\usepackage[
 pass,% keep layout unchanged
 % showframe,% show the layout
]{geometry}
\usepackage[skins]{tcolorbox}
\usepackage{verbatim}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage[export]{adjustbox}
% \usepackage{mathtools}
\setlength{\belowcaptionskip}{-10pt}
\renewcommand{\footnotesize}{\scriptsize}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
% \definecolor{lightgray}{gray}{0.8}
\definecolor{LightCyan}{rgb}{0.88,1,1}
\definecolor{darkgray}{gray}{0.7}
\definecolor{Gray}{rgb}{0.88,1,1}
\definecolor{Gray}{gray}{0.85}
\definecolor{Blue}{RGB}{0,29,193}
\definecolor{MyDarkBlue}{rgb}{0,0.08,0.45} 
\definecolor{pink}{rgb}{231,95,110}
\definecolor{lightergray}{rgb}{0.85, 0.85, 0.85}
\definecolor{darkgray}{rgb}{0.47, 0.47, 0.47}
\definecolor{lightestgray}{rgb}{0.95, 0.95, 0.95}
\definecolor{ao(english)}{rgb}{0.0, 0.5, 0.0}
% \frenchspacing
\lstset{
  language=Python,
  basicstyle=\sffamily\fontsize{2.5mm}{0.8em}\selectfont,
  breaklines=true,
  prebreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
  frame=l,
  showtabs=true,
  columns=fullflexible,
  showspaces=false,
  showstringspaces=false,
  keywordstyle=\color{pink}\bfseries\sffamily\fontsize{2.8mm}{0.6em},
  emph={train, predict, add_samples, FindBellwether, BEETLE, get_cost, sample, get_perf, remove_non_bellwethers, LinearTransform, GPTransform,General,get_featrures,BIRCH,bellwether,get_bellwethers,O(N), if, else, in,for, return}, 
  emphstyle=\bfseries\color{blue!50!black} ,
  stringstyle=\itshape\color{black!50},
  commentstyle=\color{red!50!black}\it,
  numbers=left,
  captionpos=t,
  escapeinside={\%*}{*)}
}

\newcommand{\bluecheck}{}%
\DeclareRobustCommand{\bluecheck}{%
  \tikz\fill[scale=0.4, color=blue]
  (0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;%
}

\newcommand{\response}[1]{\textcolor{blue}{#1}}
\usepackage[tikz]{bclogo}
\newenvironment{RQ}[1]%
{\noindent\begin{minipage}[c]{\linewidth}%
\begin{bclogo}[couleur=gray!20,%
                arrondi=0,logo=\none,% 
                ombre=true%
                ]{{\small  ~#1}}}%
{\end{bclogo}\vspace{2mm}\end{minipage}}

\usepackage{fancyvrb}
\fvset{%
fontsize=\small,
numbers=left
}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\eq}[1]{Equation~\ref{eq:#1}}

%%% graph
\newcommand{\crule}[3][darkgray]{\textcolor{#1}{\rule{#2}{#3}}}
\newcommand{\quartex}[3]{
\begin{picture}(13,6)%1
  {
   \color{black}
    \put(#3,3)
    {\circle*{4}}
    \put(#1,3)
    {\line(1,0){#2}}
  }
\end{picture}
}
\definecolor{lightgray}{gray}{0.7}
\tikzstyle{highlighter} = [lightgray, line width = \baselineskip]
\usepackage{wrapfig}
\newcounter{highlight}[page]
\newcommand{\tikzhighlightanchor}[1]{\ensuremath{\vcenter{\hbox{\tikz[remember picture, overlay]{\coordinate (#1 highlight \arabic{highlight});}}}}}
\newcommand{\bh}[0]{\stepcounter{highlight}\tikzhighlightanchor{begin}}
\newcommand{\eh}[0]{\tikzhighlightanchor{end}}
\AtBeginShipout{\AtBeginShipoutUpperLeft{\ifthenelse{\value{highlight} > 0}{\tikz[remember picture, overlay]{\foreach \stroke in {1,...,\arabic{highlight}} \draw[highlighter] (begin highlight \stroke) -- (end highlight \stroke);}}{}}}


\newcommand{\squishlist}{
 \begin{list}{$\bullet$}
 { \setlength{\itemsep}{0pt}
   \setlength{\parsep}{3pt}
   \setlength{\topsep}{3pt}
   \setlength{\partopsep}{0pt}
   \setlength{\leftmargin}{1.5em}
   \setlength{\labelwidth}{1em}
   \setlength{\labelsep}{0.5em} } }

\newcommand{\squishlisttwo}{
 \begin{list}{$\bullet$}
 { \setlength{\itemsep}{0pt}
  \setlength{\parsep}{0pt}
  \setlength{\topsep}{0pt}
  \setlength{\partopsep}{0pt}
  \setlength{\leftmargin}{1em}
  \setlength{\labelwidth}{1.5em}
  \setlength{\labelsep}{0.5em} } }

\newcommand{\squishend}{
 \end{list} }

\newcommand{\specialcell}[2][c]{%
 \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}

\newcommand{\flash}{{\sc Flash}\xspace}
% \usepackage{soul}
\usepackage{color}

\definecolor{awesome}{rgb}{1.0, 0.13, 0.32}


% \usepackage{amsmath}
\definecolor{Gray}{gray}{0.95}
% \definecolor{LightGray}{gray}{0.975}

\DeclareRobustCommand{\hlgreen}[1]{{\sethlcolor{green}\hl{#1}}}
\DeclareRobustCommand{\hlyellow}[1]{{\sethlcolor{yellow}\hl{#1}}}
\DeclareRobustCommand{\hlred}[1]{{\sethlcolor{awesome}\textbf{\hl{#1}}}}


%% reviewing
\newcommand{\blue}[1]{{\color{blue}{#1}}}
\newcommand{\review}[1]{\vspace{3mm}{\noindent\textit{#1}}\vspace{3mm}}
\newcommand{\reponse}[1]{\noindent{#1\\}}
\newcommand{\todo}[1]{\textbf{\color{red}{#1}}}
\newcommand{\subsect}[1]{\SS\ref{subsect:#1}}

%% Response text prefix
\newcommand{\respto}[1]{
\fcolorbox{black}{black!15}{%
\label{resp:#1}%
\bf\scriptsize R{#1}}}

%% Response text prefix
\newcommand{\bareresp}[1]{
\fcolorbox{black}{black!15}{%
\bf\scriptsize R{#1}}}

%% Cite responses
\newcommand{\citeresp}[1]{%
{(see }\fcolorbox{black}{black!15}{%
\bf\scriptsize R{#1}}~{{on page \pageref{resp:#1})}}}%
\usepackage{cite}
% \usepackage[switch, pagewise]{lineno}
% \linenumbers

\newenvironment{steelblue}
{\color{steel}}
{\color{black}}

\newenvironment{result}
{\vspace{0.15cm}
\noindent\begin{minipage}{\linewidth}
\begin{center}
\arrayrulecolor{lightergray}
\begin{tabular}{|p{0.95\linewidth}|}
\hline%
\rowcolor{lightergray}%
\textbf{Result:}~%
}
{\\\hline
\end{tabular}
\end{center}
\end{minipage}
\vspace{0.15cm}
}

\newenvironment{goal}
{\vspace{0.15cm}
\noindent\begin{minipage}{\linewidth}
\begin{center}
\arrayrulecolor{black}
\begin{tabular}{|p{0.95\linewidth}|}
\hline\rowcolor{lightestgray}}
{\\\hline
\end{tabular}
\end{center}
\end{minipage}
\vspace{0.15cm}
}

\newcommand{\quart}[4]{\begin{picture}(100,6)%1
{\color{black}\put(#2,3){\color{black}\circle*{4}}\put(#1,3){\line(1,0){#3}}}\end{picture}}
\newcommand{\quartr}[4]{\begin{picture}(100,6)%1
{\color{black}\put(#2,3){\color{red}\circle*{4}}\put(#1,3){\line(1,0){#3}}}\end{picture}}
\newcommand{\quartx}[4]{\begin{picture}(100,1)%1
{\color{black}\put(\numexpr #2 * 6  \relax,3){\color{red}\circle*{4}}\put(\numexpr #1*6 \relax ,3){\line(1,0){\numexpr #3*6 \relax}}}\end{picture}}

\usepackage{tcolorbox}
\newtcolorbox{blockquote}{colback=black!5,boxrule=0.4pt,colframe=black,fonttitle=\bfseries}

\begin{document}
\title{   Finding GENERAL    Defect Prediction Models   
Within Hundreds  of   Software Projects}


\author{Suvodeep Majumder, Rahul Krishna and Tim Menzies,~\IEEEmembership{IEEE Fellow}
%\affiliation{Computer Science, NCSU, USA, North Carolina}  
%\email{[smajumd3,rkrish11]@ncsu.edu, tim@ieee.org}

\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem S, Majumder, R. Krishna and  T. Menzies are with the Department
of Computer Science, North Carolina State University, Raleigh, USA.\protect\\
E-mail:[smajumd3,rkrish11]@ncsu.edu, tim@ieee.org}}
 

% The paper headers
\markboth{IEEE Transactions on Software Engineering, submitted Oct `19}{Majumder \MakeLowercase{\textit{et al.}}: GENERAL: Scaling Transfer Learning to 100s of Projects}

\IEEEtitleabstractindextext{%
\begin{abstract}


\respto{2-22}{\color{blue}Given a large group of software projects, there often exists  a few   exemplar project(s) that offers the best prediction for all others. Such ``bellwether projects'' can be used to make quality predictions that are general to many other projects.} 

Existing  methods  for  finding bellwether  have two problems. Firstly, they are  very slow.  When  applied  to  the  697  projects  studied  here, standard bellwether methods   took  60  days  of  CPU  to  find  and  certify  the  bellwethers.
Secondly, they assume that only one bellwether exists and, amongst hundreds of projects, they may exist  subgroups, each of which requires their own bellwether.  

GENERAL is a new bellwether method that addresses both problems. GENERAL   applies hierarchical clustering to groups of project data. At each level within a tree of clusters, one bellwether is computed from sibling projects, then promoted up the tree.  In this way, GENERAL can find multiple
bellwethers, if they exist. Also, GENERAL's hierarchical decomposition runs much faster (and scales better) than standard
bellwether methods.  

\end{abstract}

\begin{IEEEkeywords}
Transfer Learning, Bellwether, Defect Prediction, Software Analytics, Logistic Regression. 
\end{IEEEkeywords}}



% make the title area
\maketitle

\IEEEdisplaynontitleabstractindextext
\IEEEdisplaynontitleabstractindextext


\ifCLASSOPTIONcaptionsoff
 \newpage
\fi

\section{Introduction}

\respto{1-2}\respto{1-3}{\color{blue} Researchers and industry practitioners make use of data-driven decision making data mining algorithms to build software quality models from project data. Many research and industrial groups generate defect predictors from software metrics~\cite{ghotra2015revisiting, zimm09 ,okutan2014software, he13, Zhang16aa, nam2017heterogeneous, Me13, Tantithamthavorn18, me09b, me10a, peters2015lace2, nam2018heterogeneous, menzies2011local, zimmermann09}. These metrics can take  forms including process metrics (that describe how and who updates the code) and product metrics (such as the number of methods per class, lines of codes, etc). When the creation of such models is fully automated
(using data mining), then this approach scales a large number of projects. Hence, data-driven decision making  is widely used in   SE research and   industrial~\cite{tosun2010ai,kim2015remi,wan2018perceptions}. }

\begin{table*}
\begin{center}
\includegraphics[width=14cm]{figs/istable.png}
\end{center}
\caption{Contradictory conclusions from OO-metrics studies for defect prediction. From~\cite{menzies2012local}, Studies report significant (``+'') or irrelevant (``-'') metrics verified by univariate prediction models. Blank entries indicate that the corresponding metric is not evaluated in that particular study.   CBO = coupling between objects; RFC = response for class (number of methods executed by arriving messages); LCOM = lack of cohesion (pairs of methods referencing one instance variable, different definitions of LCOM are aggregated); NOC = number of children (immediate subclasses); WMC = number os methods per class. For more details on these examples, and references to the papers containing these studies, see~\cite{menzies2012local}.}
\label{unstable}
\end{table*}

{\color{blue} But what is the external validity of the conclusions reached  via these data mining methods? Prior to this paper, the usual result is that such conclusions are not general to multiple projects. Table~\ref{unstable} summarizes the results of dozens of studies. In that table, from project to project, very different attributes are most associated with defect prediction. This is troubling since,   as
Sawyer et al. argue,   project-general
insights are the key driver for businesses to invest in data analytics. If  new models keep being generated whenever we switch to  new projects, then that exhausts the ability of  users to draw insight from  new data.
Hassan~\cite{Hassan17} cautions that managers lose faith in software analytics if its models keep changing since  the assumptions used to make prior policy decisions may no longer hold. 
 Further,
if we are unsure of what factors most influence quality, it is difficult to design and implement and deploy tools that can successfully improve that quality.
Lastly,
consider the task of training a   novice software engineers or newcomers to a project. If our models are not stable, then it hard to teach what factors  most influence software quality.}





 
A new approach for transferring conclusions
across multiple projects
is     the ``bellwether'' method~\cite{krishna2018bellwethers,krishna16a,mensah18z,mensah2017stratification,mensah2017investigating}.\respto{1-1a}\respto{2-2}{\color{blue} Given a community of projects developed by the same organization,
the ``bellwether'' is a project or  a handful of projects
whose data yields the best analytic (i.e. defect prediction, effort estimation, etc) on all others}\footnote{We call that the ``bellwether'' since that is the
name of the  leading sheep of a flock, with a bell on its neck.}. In this  method:
\bi
\item We find a few exemplar ``bellwethers'';
\item We draw conclusions from  those projects.
\ei
{\color{blue} \respto{1-1a}\respto{3-5}This approach is useful since bellwethers can generate
models that hold across multiple data sets.
Also, bellwethers are a better
way to transfer models between  projects~\cite{krishna2018bellwethers,krishna2017simpler, nair19a, krishna16, krishna16a}  than many other methods including
the Burak filter~\cite{turhan09}, Ma et al.~\cite{Ma2012}'s transfer naive Bayes (TNB); and Nam et al.'s  TCA  and TCA+, algorithms~\cite{Nam13,Nam2015}.} 

% When new projects arrive, then even before there is much experience with those new projects,  models learned from other projects can be applied to the new project (just by studying the bellwether),
%  Further, \respto{2-2}{\color{blue} since the bellwether is an exemplar projects for the community of N projects seen so far},
%  %since the bellwether is similar to the other models
%  then when new projects appear, \respto{2-3}{\color{blue} bugs can be localized for that project} even before there is an extensive experience base within that particular project (again, just by studying the bellwether).
 %That is, bellwethers can be queried  to find general lessons about (a)~major quality issues;
 %or (b)~best practices to avoid quality problems;
 %or (c)~cost-benefit justifications for buying or building tools can be made stable across a large number 
 %of projects. 
 

Existing  methods  for  finding bellwether  have two problems. Firstly, they are  very slow.  When  applied  to  the  697  projects  studied  here, standard bellwether methods   took  60  days  of  CPU  to  find  and  certify  the  bellwethers.
Secondly, they assume that only one bellwether exists and, amongst hundreds of projects, they may exist  subgroups, each of which requires their own bellwether.  

GENERAL is a new bellwether method addresses both problems.  
 {\color{blue}\respto{1-3a} Our GENERAL    algorithm uses hierarchical clustering  to recursively divide a large number of projects into smaller clusters. 
 Starting at the leaves of that tree of clusters, GENERAL finds the    bellwethers within sibling projects. That bellwether is then promoted up the tree.}
%Note that  GENERAL only needs
%to compare $O(m*(N/m)^2)$  projects to find the bellwether. 

  GENERAL has three advantages over standard bellwether methods.
Firstly, 
according to several of the criteria studied here, GENERAL
can produce better predictors for software quality.  Secondly, it is much  
faster than standard bellwethers
(and scales to larger sets of data).
Thirdly, as shown below, GENERAL can find multiple bellwethers (if they are needed).

To present GENERAL and to assess its value, this
paper asks and answers the following questions.
\newpage         
             \noindent {\color{blue}
            \textbf{RQ1}: {\bf  How slow are   conventional bellwether methods?} \respto{1-6}
            As a starting point for this research,  we need to document the problems associated with existing bellwether methods. RQ1 offers the following theoretical result:}
           \vspace{-3mm}\begin{quote}
           \item {\em
              The conventional bellwether method as proposed by Krishna et al~\cite{krishna2018bellwethers,krishna16a} is slow since it requires  $ O(N^2) $ comparisons to find   one bellwether within a community of N  projects.}
        \end{quote}
             
            
         
      
            
            \noindent \textbf{RQ2}:  {\bf In theory, how much faster is GENERAL's hierarchical clustering?}        
            \begin{quote}
               {\em              GENERAL is theoretically much faster since
             its computational complexity is  $O(m*(N/m)^2)$.
            }\end{quote}
            \noindent \textbf{RQ3}:  {\bf  In practice,
            how fast is  GENERAL?} This research question checks if GENERAL tames the run-time complexity of standard bellwether. We find that:
            \begin{quote}
              \noindent {\em    
                When learning bellwethers from 697 projects,  GENERAL and   traditional   approaches terminated in 1.5 and 72 hours (respectively). That is to say,  GENERAL is far faster than standard bellwether methods.}
            \end{quote}
           \noindent \textbf{RQ4}: {\bf  Is this faster bellwether effective?}
            Sometimes,  a faster method   comes at a cost of less effective analysis. Hence, {\bf RQ4} compares
             standard and the GENERAL bellwether method. We  find that:
            \begin{quote}
              \noindent {\em  
                Across multiple performance criteria, the faster GENERAL bellwether algorithm never performs worse than standard bellwether. And measured in terms of recall and false alarm, GENERAL performs statistically much better.}
            \end{quote}
       \noindent 
            \textbf{RQ5}: {\bf Does learning from too many projects have detrimental effects?}
          In the case of GENERAL's hierarchical clustering, we show that it is a mistake to  learn quality models from the very top of the cluster tree (where data is combined from all projects). Rather, it is best to use  models seen   one level down from the root. That is:
             \begin{quote}
              \noindent {\em  
            Learning from  a few hundred  projects is better
than learning from just a few dozens of projects.
But after generalizing over 200 projects (approx), the learned
model starts degrading.  
  }\end{quote} 
\noindent To say that another way,   when learning from a very large number of projects (hundreds), they may be useful
to learn a handful of bellwethers, each of which covers (say) just a few hundred projects. In this regard,
GENERAL is better than prior bellwether algorithms
since GENERAL can find multiple bellwethers while
older methods only ever find one.
    

  
  
 
The rest of this paper is structured as follows. Some background and related work are discussed in section~\ref{sec:literature}. Our algorithm GENERAL is described in section~\ref{GENERAL} along with performance measures in section~\ref{sec:Measures}. Data collection and experimental setup are in section~\ref{sec:Data Collection}. Followed by evaluation criteria in section~\ref{eval}. The results and answers to the research questions are presented in section~\ref{sec:results}. Discussion in section~\ref{sec:rq6}, which is followed by threats to validity in section~\ref{sec:validity}. Finally, the conclusion is provided in section~\ref{sec:concl}.


 \respto{1-5d} \respto{1-5h}{\color{blue} Note that our  replication package\footnote{\href{http://tiny.cc/general\_bellwether}{http://tiny.cc/general\_bellwether}}   contains  all the datasets and scripts used in this paper.}


\respto{3-6}{\color{blue}  Before beginning, we offer
a clarification on our use of the term ``general conclusions''. 
 Like Petersen and Wohlin~\cite{Petersen2009}, we doubt
 that all projects are so similar that a single
 model describes them all.  
 Accordingly, our goal
is conclusions that hold over 
{\em many} projects, but not
necessarily {\em all projects}.
The achievement documented here is that:
\bi
\item
We can
 generate one defect predictors across
 hundreds of projects.
  \item
  Those 
  predictors perform  just as well as those found via  standard methods.
  \ei
  There are two important features of the conclusions reached via the methods of this paper:
  \bi
  \item
  Our single
  model offers 
  external  valid conclusions
  that are stable across hundreds of projects. As argued above, such a result
has not been previously reported in SE.
\item
The conclusions reached in this way used automatic data mining methods applied to open source data. That is, our results   can be repeated/ improved/ refuted by other research teams.
\ei}




 
 



\section{Background and Related Work}
\label{sec:literature}
\subsection{Why Transfer Knowledge?}
\label{sec:related}



There are many other reasons to
explore learning from many projects. Those reasons   fall into four groups:

\textbf{(a) The lesson on big data is that that the {\em more} training data, the {\em better} the learned model:} Vapnik~\cite{vapnik14} discusses examples where the model's accuracy improves to nearly 100\%, just by training on $10^2$ times as much data. This effect has yet to be seen in SE data~\cite{menzies2013guest} but that might just mean we have yet to use enough training data (hence, this study). 

\textbf{(b) We need to learn from more data since there is very little agreement on what has been learned so far:} 
% Another reason to try generalizing across more SE data is that, among developers, there is little agreement on what many issues relating to software:
\respto{2-23}{\color{blue} Among developers,  there is little agreement on what is of importance relating to software quality and other aspects of software development:}
\bi
    \item
    According to Passos et al.~\cite{passos11},  developers often  assume that the knowledge they learn from a few past projects are general to all their future projects. They comment, ``past experiences were taken into account without much consideration for their context'' ~\cite{passos11}. 
	\item
	J{\o}rgensen \& Gruschke~\cite{Jo09} offer a similar warning. They report that the suppose software engineering ``gurus'' rarely use knowledge from past projects to improve their future reasoning and that such poor past advice can be detrimental to new projects.~\cite{Jo09}.
    \item 
    Other studies have shown some widely-held views are   now questionable given new evidence. Devanbu et al. examined responses from 564 Microsoft software developers from around
	the world. They comment programmer beliefs can vary with each project, but do not necessarily
	correspond with actual evidence in that project~\cite{De16}. 
\ei
	
The good news is that using software analytics, we can correct the above misconceptions. \respto{2-15}{\color{blue} If data mining shows that higher or lower XYZ is cause for code to be  bug prone, then we could  guide developers to avoid having higher or lower XYZ. But will developers listen to us? If they ask ``are we sure having higher or lower XYZ is a statistical predictor for bug prone code?'', can we say that we have mined enough projects to ensure that XYZ is a statistical predictor for bug prone code?} 



\textbf{(c) Imported data can be more useful than local data:} Another benefit of  importing data from other projects is that, sometimes, that imported data can be better than the local information. Rees-Jones reports in one study that while building predictors
for Github close time  for open source projects~\cite{rees2017better} using data from other projects performs much better than building models using {\em local learning} (because there is better  information {\em there} than {\em here}).


\textbf{(d) When there is insufficient local data, learning from other projects is very useful:} When developing new software in  novel areas, it is useful to draw on the relevant  experience  from related areas with a larger experience base. This is particularly true when developers are doing something that is novel to them but has been widely applied elsewhere, Clark and Madachy~\cite{clark15} discuss 65 types of software they see        under-development by the US Defense Department in 2015.   Some of these types are very common (e.g. 22 ground-based communication systems) but other types are very rare (e.g. only  one avionics communication system). (e.g. workers on   flight avionics   might check for knowledge  from ground-based communications). Developers  working in an uncommon area (e.g. avionics communications) might want to transfer knowledge from more common areas (e.g. ground-based communication).

\subsection{ How to  Transfer Knowledge}\label{tion:how1}
This     art of moving data and/or knowledge  from one project or another is {\em Transfer Learning}. When there is insufficient current data to apply data miners to learn defect predictors, transfer learning can be used to transfer knowledge from other source projects S to the target project T.

Initial experiments with transfer learning offered very pessimistic results. Zimmermann et al.~\cite{zimmermann2009cross} tried to port models between two web browsers (Internet Explorer and Firefox) and found that cross-project prediction was still not consistent: a model built on Firefox was useful for Explorer, but not vice versa, even though both of them are similar applications. Turhan's initial experimental results were also very negative: given data from 10 projects, training on S = 9 source projects and testing on T = 1 target projects resulted in alarmingly high false positive rates (60\% or more). 

Subsequent research realized that data had to be carefully sub-sampled and possibly transformed before quality predictors from one source are applied to a target project. Transfer learning can have two variants - 

\bi
\item    
 \textbf{Heterogeneous Transfer Learning:} This type of transfer learning operates on source and target data that contain the different attributes~\respto{2-17}{\color{blue}\cite{jing2015heterogeneous,he2014towards,nam2017heterogeneous,cheng2016heterogeneous,yu2017feature}}.

\item
\textbf{Homogeneous Transfer Learning:} This kind of transfer learning operates on source and target data that contain the same attributes. This paper explores scalable methods for homogeneous transfer~\respto{2-17}{\color{blue}\cite{ma2012transfer,zimmermann2009cross,turhan2009relative,krishna2018bellwethers}}.
\ei
Another way to divide transfer learning is the approach that is followed. There are  2 approaches that are frequently used in many research: {\bf similarity-based approaches} and {\bf dimensional transforms}.

 \textbf{Similarity-Based Approaches:} In this approach, we can transfer some/all subset of the rows or columns of data from source to target. The Burak filter~\cite{turhan09} builds its training sets by finding the k = 10 nearest code modules in S for every $ t \in T $. However, the Burak filter suffered from the all too common instability problem (here, whenever the source or target is updated, data miners will learn a new model since different code modules will satisfy the k = 10 nearest neighbor criteria). 

    
\textbf{Dimensional Transformation:} In this approach we manipulate the raw source data until it matches the target. An initial attempt on performing transfer learning with Dimensionality transform was undertaken by Ma et al.~\cite{Ma2012} with an algorithm called transfer naive Bayes (TNB). This algorithm used information from all of the suitable attributes in the training data. Based on the estimated distribution of the target data, this method transferred the source information to weight instances the training data. The defect prediction model was constructed using these weighted training data. Nam et al.~\cite{Nam13} originally proposed a transform-based method that used TCA based dimensionality rotation, expansion, and contraction to align the source dimensions to the target. They also proposed a new approach called TCA+, which selected suitable normalization options for TCA, When there are no overlapping attributes (in heterogeneous transfer learning) Nam et al.~\cite{Nam2015} found they could dispense with the optimizer in TCA+ by combining feature selection on the source/target following by a Kolmogorov-Smirnov test to find associated subsets of columns. Other researchers take a similar approach, they prefer instead a canonical-correlation analysis (CCA) to find the relationships between variables in the source and target data~\cite{jing2015heterogeneous}.

\textbf{The Bellwether Method:}
Considering all the attempts at transfer learning sampled above, suggested a surprising lack of consistency in the choice of source datasets, learning methods, and statistical measures while reporting results of transfer learning. This issue was addressed by ``Bellwether'' suggested by Krishna et al. ~\cite{krishna2017simpler,krishna16}. which is a simple transfer learning technique is defined in 2- folds namely the Bellwether effect and the Bellwether method:

\bi

    \item \textbf{The Bellwether effect} states that, when a community works on multiple software projects,  then there exists one exemplary project, called the bellwether, which can define predictors for the others.
    
    \item \textbf{The Bellwether method} is where we search for the exemplar bellwether project and construct a transfer learner with it. This transfer learner is then used to predict for defects in future data for that community.

\ei

In their paper, Krishna et al. performed experiments with communities of 3, 5 and 10 projects in each, and showed that (a)~bellwethers are not rare, (b)~their prediction performance is better than local learning, and (c)~they do fairly well when compared with 
the state-of-the-art transfer learning methods discussed above.
%and with selection of bellwether shows a consistency for choice of source dataset for transfer learning. 
This motivated us to use  bellwethers as our choice of method for transfer learning to search for generality in SE datasets. 

There are two drawbacks with bellwethers, as seen in prior work.
Firstly, that prior work assumes that one bellwether will work for many projects. While such a solo-bellwether approach worked well
in studies over five to 20 projects, we show below that hundreds of projects might actually require more than one bellwether.

Secondly, standard bellwether algorithms are   slow.
 Krishna et al. warn that in order to find bellwether we need to do a $ N*(N-1) $ comparison; i.e. standard bellwethers
have complexity $ O(N^2) $ (N being the number of projects in the community). 

\begin{equation}
\label{eq:bellwether}
    \mathit{Bellwether\; complexity } = O(N^2)
\end{equation}

The goal of this paper is to find ways to reduce the Equation~\ref{eq:bellwether} complexity.




 
\section{About GENERAL}
% \label{sec:Metric Extraction}
\label{GENERAL}

\begin{figure}[!b]
    \small
     \begin{lstlisting}[mathescape,numbers=right,frame=None]
        def General(project):
            # getting feature vectors for each project
            # complexity is O(N)
            feature_vectors = get_features(projects)
            # hierarchically clustering projects
            # complexity is O(N)
            h_clusters = BIRCH(projects,feature_vectors)
            for level in h_clusters.levels:
                if level.depth == h_clusters.max_depth:
                    # finding bellwether for each cluster at leaf-level
                    # complexity is O(m), assuming m clusters
                    for cluster in level.clusters:
                        # finding bellwether for a cluster
                        # complexity is O(n^2) where n = N/m
                        level.cluster.bell = bellwether(cluster.projects)
                else:
                    for cluster in h_clusters.level.clusters:
                        bell_projects = cluster.child.get_bellwethers()
                        level.cluster.bell = bellwether(bell_projects)
            return h_clusters   

            \end{lstlisting} 
            \vspace{-0.2cm}
            \caption{Pseudo-code of GENERAL. Unlike traditional bellwethers (that require an $O(N^2)$ analysis of all pairs of $N$ projects, GENERAL first hierarchically generates a tree of  clusters with $m$ leaves. It  then reasons  within each cluster. This means  that GENERAL's complexity is $O(m*(N/m)^2)$.}
            \label{fig:Pseudo-code} 
            \vspace{-0.3cm}
\end{figure}

 
 Our proposed improvement to the existing bellwether method is called GENERAL. The core intuition of this new approach is that if many projects are similar, then we do not need to run comparisons between all pairs of projects.\respto{1-1b}{\color{blue}
   We came to this
intuition based on the writing
of    Devanbu
et al.`\cite{hindle2012naturalness}   who showed that a remarkably
small number of Markov chains 
capture the repeated patterns
of tokens in a program~\\
When such similarities
exist, then we   exploit symmetries between different projects in order to learn predictors from one and apply those to another. Specifically: assuming inter-project symmetry,   we do not need to run comparisons between all pairs of projects since analyzing just a few pairs should work as well as anything else. } 
%  When such similar projects exist, if may suffice to just compare a small number of representative examples. 

\respto{3-1}\respto{3-10}{\color{blue}  
 Accordingly, the rest of this paper performs the following experiment :
 \be
 \item Summarize the projects via {\bf feature extraction} (see \S\ref{sec:fx}). Assuming there are N projects, this step has a complexity of $O(N)$.
 \item Using some clustering algorithm, group all our data into sets of similar projects.
 \item The groups are themselves grouped into super-groups, then super-super-groups, etc to form a tree.  This step requires a {\bf hierarchical clustering} algorithm (see \S\ref{sec:hc}). Our hierarchical clustering algorithm BIRCH has a complexity of $O(N)$ where each project is represented by a feature vector from step 1.
 \item Implement evaluation criteria (see \S\ref{sec:Measures}).
 \item Starting at the leaves of that tree,   the best project is discovered by training on each project, and test its model
 on all others in that leaf cluster.
 This step needs
  a {\bf data mining} algorithm to generate models (see \S\ref{sec:dm})
 and  a comparison method to {\bf select the best model} (see \S\ref{sec:best}). Here if we analyze an average case scenario where $N$ projects are divided into $m$ clusters equally, then each cluster has a complexity of $O((N/m)^2)$ and the total complexity is $O(m*(N/m)^2)$ as there are $m$ clusters.
 \item  Once bellwether from each group is  pushed up the tree, each super-groups only has projects selected as bellwether from step 5 then steps 4,5 are repeated,
 recursively.
 \item The project pushed to the root of the tree is then used as  the bellwether for all the projects.
 \ee
 Note that when the clustering algorithm divides the $N$ projects into $m$ clusters at leaf level, then if we analyze the average case complexity of the algorithm using the pseudo-code from \fig{Pseudo-code} average case complexity of this method (which we call GENERAL) is:
 \begin{equation}
\label{eq:GENERAL}
    {GENERAL\; complexity } =  O(m*(N/m)^2)
\end{equation}

}
{\color{blue}
 Comparing the computational cost of Equation~\ref{eq:GENERAL} with Equation~\ref{eq:bellwether}, we can say the $O(m*(N/m)^2)$ analysis is inherently more scalable than the $O(N^2)$ analysis required by standard bellwether as m increases.}
 
To operationalize steps 1,2,3,4,5,6,7 listed above, we need to make some lower-level engineering decisions about:
\bi
\item Feature extraction;
\item Hierarchical clustering;
\item Data mining;
\item Evaluation criteria;
\item Selecting the best model.
\ei
The rest of this section documents those decisions.

\subsection{Feature Extraction}
\label{sec:fx}

Prior to anything else, we must summarize our projects as a set of metrics. Xenos \cite{Xenos} distinguishes
between {\em product metrics} (e.g.  counts of lines
of code );  and  {\em process metrics} 
about the  development process (e.g.    number of file revisions). Product metrics are properties of the code such as size, complexity, design features collected by analyzing the code, while the process metrics are properties of revision history of files collected by analyzing git history.
 \respto{2-18}{\color{blue} Here using the   Understand tool~\cite{visualize},
we calculated different variations (i.e. average, max, etc.) of 21 product and 5 process metrics, with a total of 66 attributes/columns to build defect prediction models
 (see Table~\ref{tbl:metric}).
 The Understand tool is  widely used   in software analytics~\cite{Zhang16aa,gizas2012comparative,fontana2011experience,orru2015curated,pattison2008talk,malloy2002testing}.
Using that tool, our product  metrics are
 calculated from snapshots of the projects from 6 months back of the last commit. Also, our process metrics are computed using the change history in the six-months period before the split date via manual collection of data using scripts.} The data collected for this project is summarized in \fig{lang_projects}  and \fig{meta}. Using the metrics (process and product) collected for each project we use Hall's CFS feature selector~\cite{hall1999correlation} to summarize the projects into feature vectors for the next phase of the algorithm.
 
\begin{figure}[!t]
\centering
{\scriptsize \renewcommand{\baselinestretch}{0.7}
\begin{tabular}{rrl}
    \textbf{Language} & \textbf{Projects} & \\
    Java & 290 &\crule{41.6pt}{8pt} \\
    CPP & 241 &\crule{34.5pt}{8pt} \\
    C & 116 &\crule{16.6pt}{8pt} \\
    CS & 42 &\crule{6.3pt}{8pt} \\
    Pascal & 8 &\crule{1.1pt}{8pt} \\
     & 
\end{tabular}}
\caption{Distribution of projects depending on languages.
Many  projects use combinations of languages so  we show the majority language of  each project.}
\label{fig:lang_projects}
\end{figure}

\begin{figure}[!t]
\vspace{-15mm}
    \includegraphics[angle=270,width=\linewidth]{figs/meta.pdf}
    \vspace{-15mm}
    \caption{Distribution of projects depending on Defect Percentage, Data set Size, Lines of Code and Number of Files.} 
    \label{fig:meta}
\end{figure}
 
 
\begin{table}[!t]
 
~\\

\scriptsize
\begin{tabular}{|p{1cm}|c|l|p{3cm}|}
\hline
\multicolumn{1}{|l|}{\textbf{Metric}}        & \multicolumn{1}{l|}{\textbf{Metric level}} & \textbf{Metric Name} &\textbf{ Metric Description}             \\ \hline
\multirow{21}{*}{Product   } & \multirow{6}{*}{File}             & LOC         & Lines of Code                  \\ \cline{3-4} 
                                  &                                   & CL          & Comment Lines                  \\ \cline{3-4} 
                                  &                                   & NSTMT       & Number of Statements           \\ \cline{3-4} 
                                  &                                   & NFUNC       & Number of Functions            \\ \cline{3-4} 
                                  &                                   & RCC         & Ratio Comments to Code         \\ \cline{3-4} 
                                  &                                   & MNL         & Max Nesting Level              \\ \cline{2-4} 
                                  & \multirow{12}{*}{Class}           & WMC         & Weighted Methods per Class     \\ \cline{3-4} 
                                  &                                   & DIT         & Depth of Inheritance Tree      \\ \cline{3-4} 
                                  &                                   & RFC         & Response For a Class           \\ \cline{3-4} 
                                  &                                   & NOC         & Number of Immediate Subclasses \\ \cline{3-4} 
                                  &                                   & CBO         & Coupling Between Objects       \\ \cline{3-4} 
                                  &                                   & LCOM        & Lack of Cohesion in Methods    \\ \cline{3-4} 
                                  &                                   & NIV         & Number of instance variables   \\ \cline{3-4} 
                                  &                                   & NIM         & Number of instance methods     \\ \cline{3-4} 
                                  &                                   & NOM         & Number of Methods              \\ \cline{3-4} 
                                  &                                   & NPBM        & Number of Public Methods       \\ \cline{3-4} 
                                  &                                   & NPM         & Number of Protected Methods    \\ \cline{3-4} 
                                  &                                   & NPRM        & Number of Private Methods      \\ \cline{2-4} 
                                  & \multirow{3}{*}{Methods}          & CC          & McCabe Cyclomatic Complexity   \\ \cline{3-4} 
                                  &                                   & FANIN       & Number of Input Data           \\ \cline{3-4} 
                                  &                                   & FANOUT      & Number of Output Data          \\ \hline
\multirow{5}{*}{Process }  & \multirow{5}{*}{File}             & NREV        & Number of revisions            \\ \cline{3-4} 
                                  &                                   & NFIX        & Number of revisions a file     \\ \cline{3-4} 
                                  &                                   & ADDED LOC    & Lines added                    \\ \cline{3-4} 
                                  &                                   & DELETED LOC  & Lines deleted                  \\ \cline{3-4} 
                                  &                                   & MODIFIED LOC & Lines modified                 \\ \hline
\end{tabular}
\caption{List of software metrics used in this study.}
\label{tbl:metric}
\end{table}
\subsection{Hierarchical Clustering}
\label{sec:hc}

 After data collection, comes  the  hierarchical clustering needed for step 2.
For this purpose, we followed the
advice from the scikit.learn~\cite{scikit-learn} documentation
that recommends the  Balanced Iterative Reducing and Clustering using Hierarchies  (BIRCH) algorithm for hierarchical
clustering for large sample datasets that might
contain spurious outliers~\cite{zhang1996birch}. 
BIRCH has the ability to incrementally and dynamically cluster incoming, multi-dimensional data in an attempt to maintain  best quality clustering. BIRCH also has the ability to identify data points that are not part of the underlying pattern (so it can effectively identifying and avoid outliers). 
 Google Scholar reports that  the original  paper proposing BIRCH has been cited over 5,400 times.
For this experiment we used defaults proposed by~\cite{zhang1996birch}; a   branching factor of 20 and  the ``new cluster creation'' threshold  of  0.5.

 



\subsection{Data Mining}\label{sec:dm}
The bellwether analysis of step3 requires a working data miner framework. Three requirements for that data miner framework are:
\bi
\item Since it will be called thousands of times, the learner must run quickly. Hence, we did not use any methods
that require  neural nets or ensembles. 
\item Since some projects have relatively few defects, before learning, 
some {\em over-sampling} is required to increase the number of defective examples in the training sets.
\item Since one of our motivation is to asks ``what did we learn from all these projects'', we needed a learning method that generates succinct models. Accordingly, we used {\em feature selection}  to check which subset of Table~\ref{tbl:metric}
mattered the most.
\ei
In this study, we used:
\bi
\item
Logistic regression;
\item
The SMOTE class imbalance correction algorithm;
\item
And Hall's CFS feature selector. 
\ei
We selected these tools
since   in the domain of software analytics,
the use of LR (logistic regression) and SMOTE
is endorsed by recent ICSE papers~\cite{Rahman:2013,ghotra2015revisiting,agrawal17}.
As to CFS, we found that without it, our recalls were very low (less than 20\%).
Also,   extensive studies have found that CFS more useful than many other feature subset selection methods such as  PCA or InfoGain or RELIEF\respto{1-5e} {\color{blue}~\cite{hall1999correlation,challagulla2008empirical,arar2015software,lee2016developer,rodriguez2007attribute,gao2015combining,hosni2017investigating}. }
The rest of this sub-section offers some low-level notes
on each tool.

Logistic regression  \respto{1-5a} {\color{blue}  is widely used in defect prediction~\cite{ghotra2015revisiting,zhang2017data,he2012investigation,nam2013transfer,pan2010domain} in the software engineering domain and, in our experience,
it runs much faster (in terms of model building time) than many other learners such as Support Vector Machine, Random Forest, Artificial Neural Nets etc, (when tested on defect prediction).}

\respto{1-5b} {\color{blue} The SMOTE class imbalance correction algorithm sub-samples the majority class (i.e., deletes examples) while over-sampling the minority class until all classes have the same frequency. To over-sample, new examples are synthesized extrapolating between known examples (of the minority class) and its $k$ nearest neighbors~\cite{Chawla2002}.
We use SMOTE since many of the projects have relatively very few or large number of defects. A model built on a project with such skewed class distribution can result in a biased model towards the majority class (Such models can either have a low recall or high recall with high pf). Thus we run SMOTE~\cite{tan2015online,wang2013using,sun2012using,khoshgoftaar2010attribute} on the training data\footnote{Technical aside: While it is useful
  to artificially boost the number of target examples
in the training data~\cite{Chawla2002,Pelayo2007,mensah2017investigating}, it is a methodological error to also change the distributions in   the test data~\cite{agrawal17}. Hence, for our work,
we take care to {\em only} resample the training data.}}.

\respto{1-5c} {\color{blue} Hall's
CFS feature selector~\cite{hall1999correlation} 
is based on the heuristic
that ``good feature subsets contain features highly correlated with the classification, yet uncorrelated to each other''. Using this heuristic,
CFS performs a best-first search  to discover interesting sets of features.
Each subset is  scored via
$\mathit{merit}_s = kr_{\mathit{cf}}/\sqrt{k+k(k-1)r_{\mathit{ff}}}$
where
$\mathit{merit}_s$ is the value of some subset $s$ of the
features containing $k$ features; 
$r_{\mathit{cf}}$ is a score describing the connection of that feature
set to the class;
and $r_{\mathit{ff}}$ is the mean score of the feature to feature
connection between the items in $s$.
Note that for this to be maximal, $r_{\mathit{cf}}$  must be large
and $r_{\mathit{ff}}$ must be small (that is, features have to connect more to the class than each other). We use CFS  to identify features that are highly correlated with the classification, yet uncorrelated to each other. The features which are  highly correlated with each other will not add more information while making classification, while the features which are uncorrelated towards the classification also not add more information while making classification.  Our  CFS   incrementally picks one feature at a time, stopping when there is no more improvement in the score. }






\subsection{Implement Evaluation Criteria}
\label{sec:Measures}

In this section, we introduce the following 5 evaluation measures used in this study to evaluate the performance of machine learning models. Suppose we have a dataset with M changes and N defects. After inspecting 20\% LOC, we inspected $m$ changes and found $n$ defects. Also, when we find the first defective change, we have inspected k changes. Using
this data, we can define 5 evaluation measures  as follows.


{\color{blue}\respto{1-5f} 
(1) \textbf{Recall:} This is the proportion of inspected defective changes among all the actual defective changes; i.e. $n/N$. This implicates in an ideal scenario where we identify all the actual target class as the target class without missing the recall will be one. When recall is maximal, we are finding all  the target class. Recall is used in  many previous studies~\cite{kamei2012large,yang2016effort,yang2017tlel,xia2016collective,yang2015deep}.  

(2) \textbf{Precision:} This is the proportion of inspected defective changes among all the inspected changes; i.e. $n/m$. A low Precision indicates that developers would encounter more false alarms, which may have a negative impact on developers' confidence in the prediction model. When precision is maximal, all the reports of defect modules are actually buggy (so the users waste no time looking at results that do not matter to them)

(3) \textbf{Pf:} This is the proportion of all suggested defective changes which are not actual defective changes among all the suggested defective changes. A high {\em pf} suggests developers will encounter more false alarms which may have a negative impact on developers' confidence in the prediction model.

(4) \textbf{Popt20:} This is the proportion number of suggested defective changes among all suggested defective changes, when 20\% LOC modified by all changes are inspected. A high {\em popt20} values mean that developers can find most bugs in a small percent of the code, thus effectively reducing the effort required to find bugs. To compute Popt20, we divided the test set into the modules predicted to be faulty (set1) and predicted to be bug-free (set2). Each set was then sorted in ascending order by lines of code.  We then ran down set1, then set2, till 20\% of the total lines of code was reached-- at which point {\em popt20} is the percent of buggy modules seen up to that point.

(5) \textbf{Ifa\_auc:} Number of  initial false alarms encountered before we find the first defect. Inspired by previous studies on fault localization~\cite{parnin2011automated, kochhar2016practitioners, xia2016automated}, we caution that if the top-k changes recommended by the model are all false alarms, developers would be frustrated and are not likely to continue inspecting the other changes. Parnin and Orso ~\cite{parnin2011automated} found that developers would stop inspecting suspicious statements, and turn back to traditional debugging if they could not get promising results within the first few statements they inspect. Using the nomenclature reported about {\em ifa$=k$}.  In this study, we use a modified version of {\em ifa} called ifa\_auc, which calculates {\em ifa} based on efforts spent on inspecting the code. We use gradually increment the efforts spent by increasing the total LOC inspected and calculate ifa on each iteration to get the area under the curve (auc), here the x-axis is the percentage of effort spent on inspection and y-axis is {\em ifa}.}


\subsection{Select the Best Model}\label{sec:best}
% As discussed below, the defect models assessed in these experiments 

The previous section listed numerous goals that could be used to guide the learning. Combining
numerous goals is often implemented as a multi-objective optimization predicate  one model is better than another if it
 satisfies a ``domination predicate''.
 We use the Zitler indicator dominance
 predictor~\cite{zit02} to select our bellwether (since this is known to select better models
 for multi-goal optimization in SE~\cite{Sayyad:2013,Sayyad:2013:SPL}).
This predicate favors model
  $y$ over $x$  model if $x$ ``losses'' most:
\begin{equation}\label{eq:cdom}
\begin{array}{rcl}
\textit{worse}(x,y)& =& \textit{loss}(x,y) > \textit{loss}(y,x)\\
\textit{loss}(x,y)& = &\sum_j^n -e^{\Delta(j,x,y,n)}/n\\
\Delta(j,x,y,n) & = & w_j(o_{j,x}  - o_{j,y})/n
\end{array}
\end{equation}
where  ``$n$'' is the number of objectives (for us, $n=5$) and $w_j\in \{-1,1\}$ depending on whether
we seek to maximize goal $x_j$.  

An alternative to the Zitler indicator is    `boolean domination '' that says one thing is better than another it if it no worse on any criteria and better on at least one criteria. We prefer Equation~\ref{eq:cdom} to boolean domination since we have a   3-goal optimization problem and it it is known that boolean domination often  fails for 2 or more goals~\cite{Wagner:2007,Sayyad:2013}. 
 
{\color{blue}\respto{1-5g} As discussed above, model performance can be scored and the best model can be chosen by researchers based on their choice of metrics. In this experiment, we score model performance according to the goals mentioned in \S\ref{sec:Measures}. Here when assessing models, we  chose the best model that:}

\bi
\item
{\color{blue}Maximize
recall and precision and popt(20)}
\item {\color{blue} Minimizes false alarm and ifa\_auc.}
\ei


{\color{blue}
\noindent
To summarize, the algorithm works as follows -
\bi
\item The feature extraction algorithm extracts feature vectors for each project.
\item A hierarchical clustering algorithm BIRCH, uses the feature vectors for each project as representative for the project to create a hierarchical cluster. This returns a clustering tree, which includes information regarding which projects are in a certain cluster, each cluster's child clusters and parent cluster.
\item Use the clustering tree to find clusters in leaf level of the tree, to run an all pair comparison within each cluster to select a best model aka ``Bellwether''. 
\item We push up and store the bellwether found at leaf level to its parents.
\item On the parent clusters, we find the best models pushed from child nodes to run an all pair comparison between them to find the ``Bellwether'' in the parent clusters.
\item We repeat the last 2 steps until we reach the root node.
\item When we predict defects for a new project, we first check which cluster in each level it should belong to based on the BIRCH algorithm and then we predict the new project using that cluster's ``Bellwether'' at each level.
\ei
}

\section{Experimental Methods}
\label{sec:Data Collection}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{figs/General.pdf}
    \caption{ Experimental rig for this paper.  In this rig, 
      bellwethers are
    learned and tested on {\em separate } projects. Within the test set (denoted ``test1``, above),
    the data is further divided into ``train2, test2``. To assess the bellwether found from ``train1`` against local learning, data from each project is divided into ``train2,test2``
    then (a)~local
    models are learned from   ``train2``; after which time,
    (b)~the local models from ``train2'' and the bellwether model from ``train1``
    are both applied to the same data from ``test2``.
    Note that this process is repeated 20 times, with different random number seeds, to generate 20 different sets of ``train1, train2, test2``.
    }
    \label{fig:GENERAL}
\end{figure*}

\subsection{Data Collection}
\label{sec:data}

To perform our experiments we choose to work with defect prediction datasets. We use 
the data collected by Zhang et al.~\cite{zhang15}.
This data has the features of Table~\ref{tbl:metric}.
Originally, this data  was collected by Mockus et al.~\cite{mockus2009amassing} from SourceForge and GoogleCode. The dataset contains the full history of about 154,000 projects that are hosted on SourceForge and 81,000 projects that are hosted on GoogleCode to the date they were collected. In the original dataset each file contained the revision history and commit logs linked using a unique identifier. Although there were 235K,000 projects in the original database, many of there are
trivially small or are about  non-software development projects. Zhang et al. cleaned the dataset using the following    criteria:


\bi
  \item \textbf{Avoid projects with a small number of commits:} Zhang et al. removed any projects with less than 32 commits (which is the 25 \% quantile of the number of commits as the threshold). 
 
    
    \item \textbf{Avoid projects with a lifespan of less than one year:} Zhang et al. filtered out any  projects with a lifespan of less than one year.  
    
    \item \textbf{Avoid projects with limited defect data:} Zhang et al. in their study counted the number of fix-inducing and non-fixing commits from a one-year period and removed any projects with 75 \% quantile of the number of fix-inducing and non-fixing commits.  
    
    \item \textbf{Avoid projects without fix-inducing commits:} Zhang et al. filtered out projects that have no fix-inducing commits during six months as abnormal projects, as projects in defect prediction studies need to contain both defective and non-defective commits.
\ei
On top of that, we also applied two more filters:
\bi
    \item \textbf{Use mainstream programming Languages:} the tool
    we used (Understand~\cite{visualize}) only supported     mainstream languages in widespread
    industrial use; specifically: object-oriented languages with file extension i.e *.c, *.cpp, *.cxx, *.cc, *.cs, *.java, and *.pas.
    
    \item \textbf{Avoid projects with less than 50 rows:} We removed any project with less than 50 rows as they are too small to build a meaningful predictor. 
       \item \textbf{Avoid projects with too few errors:}
    We pruned  projects which did not  have enough fix-inducing vs non-fixing data points to create a stratified k=5 fold cross-validation/% an 
    
\ei
These filters resulted in a training set of   697 projects\footnote{\href{http://tiny.cc/general\_data}{http://tiny.cc/general\_data}}. Fig~\ref{fig:meta} and fig~\ref{fig:lang_projects} shows the Distribution of projects depending on defect percentage, data set size, lines of code, number of files and project languages to confirm the projects selected comes from wide verity and representative of a software community. 
From these selected projects, the data was labeled using issue tracking system and commit messages. If a project used issue tracking system for maintaining issue/defect history the data was labeled using that. Like  Zhang et al., we found that nearly half  of the projects did not use an issue tracking system. For these projects, labels were created analyzing commit messages by tagging them as fix-inducing commit if commit message matches the following regular expression

\begin{center}
\textit{(bug $\mid$ fix $\mid$ error $\mid$ issue $\mid$ crash $\mid$ problem $\mid$ fail $\mid$ defect $\mid$ patch)}
\end{center}
 




\subsection{Experimental Setup}
\label{sec:Experimental}


\fig{GENERAL} illustrates our experimental rig. 
 The following process was repeated 20 times, with different random seeds used each time.
 \bi
 \item
 Projects were divided
 randomly into train\_1 and test\_1 as a 90:10 split (from a total of 697 projects, 627 projects are for train\_1 and remaining 70 for test\_1).
 \item
 The projects in train\_1 were used to find the bellwether.
 \item
 Each   project in test\_1 was  then divided into train\_2 and test\_2 (using a 2:1 split).
 \item
 LR and feature selection and SMOTE were then used
 to build two models: one from the train\_1 bellwether and one from the train\_2
 data.
 \item
 Both models were then applied to the test\_2 data.
 \ei
 






 
\subsection{Learners}\label{mylearners}

In this study, we applied the following learners:

%

\textbf{Self:} (a.k.a. local learning). This  is the standard method used in software analytics~\cite{menzies2013software,zhang2013software}.
In this approach, the local project data is divided into a 90\% training set (which we call  train\_2)
and a 10\% test set (which we call test\_2). After that, some learner builds a model from the training data
(using the methods of \S\ref{sec:Experimental}),
which is then assessed on the test data.

As we shall see, this approach produces competent defect predictors.
Recalling
the motivation of this paper: we do not
seek a better predictor that is (say) more accurate
than {\bf self}.
That is, hierarchical bellwethers can be recommended even if they {\em 
perform  no better  than {\bf self}}. Rather, our goal is to test if hierarchical bellwethers
can quickly  find a  small set of {\em adequate conclusions}
that hold across a large space 
of projects
(and here, by ``adequate'', we mean conclusions
that perform no worse than those found by other methods).

\textbf{ZeroR:} 
In his textbook on ``Empirical AI", Cohen~\cite{Cohen:1995} recommends base-lining new methods against some simpler approach.
For that purpose, we use {\bf ZeroR} learner. This learner assigns labels every test instance according to
the majority class of the training data. Note that if anything we do performs worse that ZeroR, then there is no
point in any of the learning technology explored in this paper.  

 
\textbf{Global:}  Another baseline, against which we compare our methods
is a {\bf Global} learner build using all the data   train\_1.
Note that, if this learner performs best, then this  would  mean that we could replace GENERAL with a much simpler system.


\textbf{Bellwether0:} This learner is the  $O(N^2)$ bellwether method  proposed by Krishna et al.~\cite{krishna16a}.
What will we show is that GENERAL does better than {\bf Bellwether0} is three ways:
(a) GENERAL is inherently more scalable; (b) GENERAL is (much) faster, and; (c) GENERAL produced better predictions.
That is, our new GENERAL method is a significant improvement over the prior state-of-the-art.


\textbf{GENERAL\_level2:}  GENERAL finds bellwethers at various levels of the BIRCH cluster tree.  GENERAL\_level2 results show the performance of the model learned from the bellwether found in the leaves of the  BIRCH cluster tree.
That is these results come from  a bellwether generated from 15 to 30 projects.  
For this process:
\bi
\item
First, we tag each leaf cluster with its associated bellwether;
\item
Second, we use the test procedure built into BIRCH; i.e. a test case is presented to the root
of the cluster tree and BIRCH returns its {\em relevant  leaf};
i.e. the cluster closest to that test case.
\item
We then apply the bellwether tagged at that leaf.
\ei
\textbf{GENERAL\_level1:}  GENERAL\_level1 results show the performance of the model learned from the bellwether found between the root and   the leaves of the  BIRCH cluster tree. In practice, BIRCH divides our data only twice
so there is only one GENERAL\_level1  between root and leaves. For this process,
we use the same procedure as GENERAL\_level2 but this time, we use the bellwether tagged in the {\em parent}
cluster of the relevant leaf. Note that these level1 results come from an analysis of between 50 to 200  projects
(depending on the shape of the cluster tree generated via BIRCH). 



\textbf{GENERAL\_level0:}  
In the following,  the  GENERAL\_level0 results show the performance of the model learned from the bellwether found at the root of the BIRCH cluster tree. Note that these results come from an analysis of over 600 projects. 

 
\subsection{Statistical Tests}
% \label{stats}
\label{eval}

When comparing the results of different models in this study, we used a statistical significance test and an effect size test:
\bi
\item
Significance test is useful for detecting if two populations
differ merely by random noise. 
\item
Effect sizes are useful for checking that two populations differ by more than just a trivial amount.
\ei
For the significance test,  we use the Scott-Knott procedure  recommended at TSE'13~\cite{mittas2013ranking} and ICSE'15~\cite{ghotra2015revisiting}. This technique recursively bi-clusters a sorted set of numbers. If any two clusters are statistically indistinguishable, Scott-Knott reports them both as belonging to the same ``rank''.

To generate these ranks, Scott-Knott first looks for a break in the sequence that maximizes the expected values in the difference in the means before and after the break. More specifically,  it  splits $l$ values into sub-lists $m$ and $n$ in order to maximize the expected value of differences  in the observed performances before and after divisions. For e.g., lists $l,m$ and $n$ of size $ls,ms$ and, $ns$ where $l=m\cup n$, Scott-Knott divides the sequence at the break that maximizes:
\begin{equation}
    E(\Delta)=\frac{ms}{ls}\times abs(m.\mu - l.\mu)^2  + \frac{ns}{ls}\times abs(n.\mu - l.\mu)^2
\end{equation}
Scott-Knott then applies some statistical hypothesis test $H$ to check if $m$ and $n$ are significantly different. If so, Scott-Knott then recurses on each division. For this study, our hypothesis test $H$ was a conjunction of the A12 effect size test (endorsed by \cite{arcuri2011practical})  and non-parametric bootstrap sampling \cite{efron94}, i.e., our Scott-Knott divided the data if {\em both} bootstrapping and an effect size test agreed that the division was statistically significant (90\% confidence) and not a ``small'' effect ($A12 \ge 0.6$).


\subsubsection{Interpreting the Scott-Knott Results}\label{sec:isk}

{\color{blue} Scott-Knott divides a set of sorted numbers into ranks $1..x$  from lowest to the highest value. These ranks have a different interpretation, depending on whether we seek to minimize or maximize those numbers. For our purposes:
\bi
\item The first rank=1 is {\em worst} for recall, precision, Popt20 since we want to {\em maximize} these numbers.
\item
The first rank=1 is {\em best} for false alarms and ifa\_auc since we want to {\em minimize} those kinds of errors
\ei
\respto{3-9}
Note one special case of the above. In the following results, we will see that ZeroR rarely predicts anything as defective. hence, it has a very low recall (median=0\%) but {\em also} a  very low false alarm and very low ifa\_auc rate. That is to say, ZeroR makes no mistakes since it rarely suggests anything at all.
Accordingly, we will not be recommending ZeroR for defect prediction.}




\section{Results}
\label{sec:results}

{\color{blue} 
\subsection*{RQ1:  How slow are   conventional bellwether methods? }
\label{sec:rq1}
\respto{1-6a} Conventional bellwether as proposed by Krishna et al. compares all projects in a given community with all others to build defect prediction models and compare with each other. This operation is quite expensive, which has an $ O(n^2) $ computational complexity. In this experiment, we used this conventional bellwether to make a baseline comparison with 697 projects (627 projects in training and 70 projects in testing) in the community. Figure~\ref{fig:compare} showed that the conventional bellwether did a total of {\em 393129} comparisons, while taking {\em 72} hours (with SMOTE and CFS) or {\em 10648} seconds (without SMOTE and CFS) to complete. In summary we conclude 

\vspace{2mm}
   \begin{blockquote}
              \noindent {\bf
              The conventional bellwether method as proposed by Krishna et al~\cite{krishna2018bellwethers,krishna16a} is slow since it requires  $ O(N^2) $ comparisons to find   one bellwethers within a community of N  projects.}
            \end{blockquote}}

{\color{blue} 
\subsection*{RQ2: In theory, how much faster is GENERAL's hierarchical clustering?}
\label{sec:rq2}
The new bellwether method proposed in this paper uses a hierarchical clustering, then finds bellwether at leaf level clusters and pushes up bellwethers to parent nodes for comparisons. This process reduces the computational complexity to $O(m*(N/m)^2)$ on an average case (assuming the hierarchical clustering creates m cluster at leaf level). In theory, this is faster than the $O(N^2)$ computation reported for {\bf RQ1}. When compared with conventional bellwether, Figure~\ref{fig:compare} shows the new bellwether only requires {\em 9583} comparisons\, which is an order of magnitude fewer comparisons. Figure~\ref{fig:compare_time}   shows the new method only takes {\em 362} seconds (without SMOTE and CFS) to complete and {\em 1.5} hours (with SMOTE and CFS). In summary, we can say:} 

\begin{blockquote}
              \noindent {\bf              GENERAL is theoretically much faster since
             its computational complexity is  $O(m*(N/m)^2)$.
            }\end{blockquote}

\subsection*{RQ3:  In practice,
            how fast is  GENERAL?}
\label{sec:rq3}


 
\respto{1-6b}{\color{blue} \fig{Pseudo-code} showed that, theoretically,
GENERAL is an inherently faster approach than traditional bellwether methods.
To test that theoretical
conclusion,
we ran the rig of \fig{GENERAL} on
a four core machine running at 2.3GHz with 8GB of RAM.} 





\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figs/Time_Compare.pdf}
    \caption{Mean run-time required for finding a bellwether in a single run of standard bellwether and GENERAL with different community size.}
    \label{fig:compare_time}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figs/compare.pdf}
    \caption{Number of comparisons required for finding a bellwether in a single run of standard bellwether and GENERAL with different community sizes.}
    \label{fig:compare}
\end{figure}




{\color{blue}\fig{compare_time} and \fig{compare} shows the mean number of comparisons and mean run time required for finding a bellwether, using conventional bellwether method versus GENERAL for different community size of 45,90, 180, 360 and 627 projects. We can see from \fig{compare_time} and \fig{compare} for increasing community size the number of comparisons and time required increases rapidly for conventional bellwether method, while with GENERAL the number of comparisons required is relatively small. This shows the new bellwether method (aka GENERAL) is scalable with increasing community size. }



In this study, we also recorded the mean run times for a single run of GENERAL versus traditional bellwether (with SMOTE and CFS) for a community size of 627 projects. For certification purposes, all experiments had to be repeated 20 times. In that certification run:

\bi
\item
The $O(N^2)$ analysis of the traditional {\em bellwether0} approach needed 60 days of CPU time.
\item 
The $O(m*(N/m)^2)$ analysis
of GENERAL needed 30  hours. That is, in empirical results consistent with the theoretical predictions of Figure~\ref{fig:Pseudo-code}, GENERAL runs much faster than the traditional bellwether.
\item 
All the other methods required another 6 hours of computation. 
\ei

If we were merely seeking conclusions from one project, then we would recommend ignoring bellwethers and just use results from each project. That said, we still endorse bellwether method since  we seek knowledge that holds across many projects.

In summary, based on these results, we conclude that:

\begin{blockquote}
              \noindent {\bf    
                When learning bellwethers from 697 projects,  GENERAL and   traditional   approaches terminated in 1.5 and 72 hours (respectively). That is to say,  GENERAL is far faster than standard bellwether methods.}
            \end{blockquote}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%Latex Table%%%%%%%%%%%%%%%%%%%%%%%%% \rowcolor[gray]{.9}
\begin{figure}[!b]
{\scriptsize
{\scriptsize 
\resizebox{.48\textwidth}{!}{
\begin{tabular}{p{.1cm}lp{1.5cm}rrc}
\arrayrulecolor{darkgray}
\rowcolor[HTML]{C0C0C0}  & rank & treatment & median & IQR & \\ 
 \multirow{5}{*}{\rotatebox[origin=c]{90}{Recall}} 
  & 1 &      ZeroR &    0 &  100 & \quart{0}{0}{100}{100} \\
  & 1 &      GENERAL\_level2 &    33 &  63 & \quart{0}{33}{63}{30} \\
  & 1 &      bellwether0 &    38 &  55 & \quart{5}{38}{55}{22} \\
  & 2 &      GENERAL\_level1 &    50 &  54 & \quart{20}{50}{54}{24} \\
  & 3 &      GENERAL\_level0 &    57 &  47 & \quart{36}{57}{47}{25} \\
  & 3 &      self &    57 &  34 & \quart{41}{57}{34}{17} \\
  &\rowcolor[gray]{0.9} 4 &      global &    100 &  12 & \quart{88}{100}{12}{0} \\ \hline
\multirow{5}{*}{\rotatebox[origin=c]{90}{Pf}} 
  & 1 &      ZeroR &    0 &  100 & \quart{0}{0}{100}{100} \\
  & 1 &      GENERAL\_level2 &    21 &  43 & \quart{0}{21}{43}{22} \\
  & 1 &      bellwether0 &    25 &  45 & \quart{0}{25}{45}{20} \\
  & 1 &      self &    25 &  23 & \quart{17}{25}{23}{15} \\
  &\rowcolor[gray]{.9} 1 &      GENERAL\_level1 &    33 &  41 & \quart{10}{33}{41}{18} \\
  & 2 &      GENERAL\_level0 &    45 &  43 & \quart{24}{45}{43}{22} \\
  & 3 &      global &    100 &  17 & \quart{83}{100}{17}{0} \\ \hline
\multirow{5}{*}{\rotatebox[origin=c]{90}{Precision}} 
  & 1 &      ZeroR &    0 &  45 & \quart{0}{0}{45}{45} \\
  & 2 &      global &    33 &  45 & \quart{11}{33}{45}{23} \\
  & 2 &      bellwether0 &    36 &  63 & \quart{4}{36}{63}{31} \\
  & 2 &      GENERAL\_level2 &    36 &  65 & \quart{0}{36}{65}{29} \\
  &\rowcolor[gray]{.9} 2 &      GENERAL\_level1 &    41 &  54 & \quart{13}{41}{54}{26} \\
  & 2 &      GENERAL\_level0 &    41 &  47 & \quart{20}{41}{47}{26} \\
  & 2 &      self &    50 &  44 & \quart{28}{50}{44}{23} \\ \hline
\multirow{5}{*}{\rotatebox[origin=c]{90}{Popt20}} 
  & 1 &      ZeroR &    0 &  28 & \quart{0}{0}{28}{28} \\
  & 2 &      GENERAL\_level2 &    28 &  41 & \quart{0}{28}{41}{12} \\
  & 2 &      global &    28 &  14 & \quart{20}{28}{14}{5} \\
  & 2 &      bellwether0 &    31 &  42 & \quart{0}{31}{42}{11} \\
  & 2 &      GENERAL\_level0 &    31 &  16 & \quart{23}{31}{16}{8} \\
  & 2 &      GENERAL\_level1 &    31 &  23 & \quart{17}{31}{23}{9} \\
  &\rowcolor[gray]{.9} 3 &      self &    36 &  20 & \quart{27}{36}{20}{11} \\ \hline
\multirow{5}{*}{\rotatebox[origin=c]{90}{ifa\_auc}} 
  &\rowcolor[gray]{.9} 1 &      ZeroR &    0 &  11 & \quart{0}{0}{11}{11} \\
  & 2 &      global &    15 &  29 & \quart{6}{15}{29}{20} \\
  & 3 &      GENERAL\_level2 &    21 &  21 & \quart{11}{21}{21}{11} \\
  & 3 &      bellwether0 &    22 &  21 & \quart{13}{22}{20}{11} \\
  & 3 &      self &    22 &  17 & \quart{15}{22}{17}{9} \\
  & 3 &      GENERAL\_level1 &    23 &  20 & \quart{14}{23}{20}{11} \\
  & 3 &      GENERAL\_level0 &    24 &  21 & \quart{14}{24}{21}{12} \\
\end{tabular}}}
}
\caption{Statistical Results comparison. The ``rank`` column at left comes from
the statistical analysis methods of \S\ref{eval}.
In this figure, the {\bf best} ranks are highlighted in gray,
As discussed in \S\ref{sec:isk}, since we want to minimize {\em pf}, and {\em ifa\_auc} then the {\em lowest ranks} (i.e. rank=1) are   {\em best}. But for other measures,
which we want to maximize (recall, precision, Popt20), the {\em largest ranks} are {\em best}. 
}\label{fig:Statistical}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%End Latex Table%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!b]
    \centering
    \includegraphics[width=.9\linewidth]{figs/bubble.pdf}
    \caption{Example of Hierarchical Clustering for 627 projects}
    \label{fig:example tree}
\end{figure*}



 

\subsection*{RQ4: Is this faster bellwether effective?}
\label{sec:rq4}

The speed improvements reported in {\bf RQ3} are only useful
of this faster method can also deliver adequate predictions
(i.e. predictions that are not worse
than those generated by other methods).

 
Figure~\ref{fig:Statistical} shows the distribution of
performance score results seen in \fig{GENERAL}. These results are grouped together by the ``rank'' score
of the left-hand-side column 
(and this rank was generated using the statistical methods of
\S\ref{eval}).


In these results, the {\em ifa\_auc}  and {\em precision} scores
were mostly uninformative. With the exception of {\em ZeroR},
there was very little difference in these scores.




As to {\em ZeroR}, we cannot recommend that approach.
ZeroR is a method where we predict everything as the majority class.
In Figure~\ref{fig:Statistical}, ZeroR model has a median recall, precision and pf of zero. 
That is, this method has very few errors since it offers very few suggestions on what might be defective.
As discussed at the end of   \S\ref{sec:isk}, we cannot recommend such a method.
  

Similarly, we cannot recommend the    {\em global} approach.
In this approach, quality predictors are learned from   one data set that combines
data from hundreds of projects. As seen in Figure~\ref{fig:Statistical} 
that approach generates an unacceptably large false alarm rate ($pf=79\%$).

Another approach we would deprecate is the traditional bellwether approach.
By all the measures of
Figure~\ref{fig:Statistical},   the {\em bellwether0} is in the middle of the pack. That is:
\bi
\item
That approach is in no way outstanding. 
\item
So, 
compared to hierarchical bellwether,
 there is no evidence here of better performance
  from    traditional bellwethers.
\item
Given this weak performance, 
and   the {\bf RQ3} results (where traditional bellwether ran very slowly), we, therefore,  
deprecate the traditional bellwether approach.
 \ei
As to GENERAL vs the local learning results of {\em self}, in many ways their performance in  Figure~\ref{fig:Statistical},
is indistinguishable:
\bi
\item As mentioned above, measured in terms of {\em ifa\_auc}
and {\em precision}, there is no significant differences.
\item For {\em recall} there is no statistical difference in the rank
of  local learning with {\em self} and 
  {\em GENERAL\_level0}
(a bellwether
generated from the root of a BIRCH cluster tree); 
\item In terms of {\em pf} (false alarms) and Popt20, some of the GENERAL results are ranked
the same as {\em self} (and we will expand on this point, below).
\ei
In part, we summarize Figure~\ref{fig:Statistical} results as follows:


%XXX GL1: worse at nothing, statistically better of recall and pf

% 5XXX RQ5 is very short.

 \begin{blockquote}
              \noindent {\bf  
                Across multiple performance criteria, the faster GENERAL bellwether algorithm never performs worse that standard bellwether. And measured in terms of recall and false alarm, GENERAL performs statistically much better.}
            \end{blockquote}
 
 This summary does not address an important issue, discussed in {\bf RQ5}.
 
 
 


\begin{figure*}[!b]
    \centering
    \includegraphics[width=\linewidth]{figs/fss.pdf}
    \caption{Distribution of features selected using self model and ``Bellwether'' model. Bars in red denote features found in the GENERAL\_level1 models (see Table~\ref{tbl:coefs_4}). }
    \label{fig:FSS_level1}
\end{figure*}

\subsection*{RQ5: Does learning from too many projects have detrimental effect?}
\label{sec:rq5}

\respto{2-21}{\color{blue}
This section ask if conclusions always improve as we learn from more and more data.
This can be an issue since if a new project
is fundamentally different from all past projects, then irrelevant ideas from the past will be
inappropriately applied to the present situation.  
This is a special problem that arises when combining old data  with new data.} 
 

To answer {\bf RQ5}, we first note that when GENERAL
calls the BIRCH clustering algorithm, it generates the tree
of clusters shown in  
\fig{example tree}. In that tree:
\bi
\item
The bellwether found at level 0 of the tree (which we call {\em GENERAL\_level0}) is learned from 627 projects.
\item
The bellwethers found at level 1 of the tree
(which we call {\em GENERAL\_level1}) is learned from four  groups
of   projects.
\item
The bellwethers found at level 2 of the tree
(which we call {\em GENERAL\_level2}) are learned from 58   groups of   projects.

\ei
From this list, we make two comments:
\bi
\item Firstly, we note if we just use the results from  
{\em GENERAL\_level0} then we are returning just one model. However, if we use the results from {\em GENERAL\_level1}, then we will return a  handful
of bellwethers. In this regard, GENERAL is different from traditional bellwethers since this new method can return multiple models, each specialized for a different set of projects.
\item
Second, we note that to answer {\bf RQ5} we need only compare the predictive performance of models learned from these different levels. In that comparison, if the level\_$(i+1)$ bellwethers generated better predictions that the   level\_$(i)$ bellwethers, then we would conclude that it is best to learn knowledge from smaller groups of projects.
\ei
Figure~\ref{fig:Statistical} lets us compare the performance of the bellwethers learned from different levels:
\bi
\item The {\em ifa} and {\em Popt20} 
and {\em precision}  results for the different levels
are all ranked the same.  Hence we say that, measured in terms of those 
measures, we cannot distinguish the performance at different levels.
\item 
As to {\em recall},   the level2,1,0 bellwether results are 
respectively ranked worst, better, best. 
\item
Worse results are seen in the {\em pf} false alarm rates using models learned from the top of the cluster tree.
\ei
In summary, measured in terms of false alarm rates,
our results say that it is possible to learn from too much data. Based on Figure~\ref{fig:Statistical}, we say that it is better to use the multiple models generated by
{\em GENERAL\_level1} since this has a lower false alarm
rate than the single model
generated by {\em GENERAL\_level0}. Hence, for this research question, we conclude:

   \begin{blockquote}
              \noindent {\bf  
            Learning from  a few hundred  projects is better
than learning from just a few dozens   projects.
But after generalizing over 200 projects (approx), the learned
model start degrading.  
  }\end{blockquote} 

To say that another way,   when learning from a very large number of projects (hundreds), they may be useful
to learn a handful of bellwethers, each of which covers (say) just a few hundred projects. In this regard,
GENERAL is better than prior bellwether algorithms
since GENERAL can find multiple bellwethers while
older methods only ever find one.




\section{Discussion}
\label{sec:rq6}

One lesson learned from this work is that
features that seem useful (based on some local analysis),
may not actually be generally useful
(based on a more global analysis).
To see this, we need to compare the
features seen in
Table~\ref{tbl:coefs_4} and
\fig{FSS_level1}  

  Table~\ref{tbl:coefs_4} 
 shows what  features were generally
 ``globally important'' 
 Here, by ``globally important'' we mean that the features were found in
  one of the four models generated from  {\em GENERAL\_level1}. Also,
  the $\beta$ coefficient on those features was not  close to zero (where ``close'' means $-0.1 \le \beta \le 0.1$).
We   say that the  features  of Table~\ref{tbl:coefs_4}   are ``globally important''
  since our analysis shows that, across many data sets,
  they are the ones that most predict for defects.
  
\fig{FSS_level1} shows what features were found to be ``locally important'' within specific data sets. 
 Recalling \S\ref{sec:dm}, we say that a feature is
 ``locally important'' if the CFS feature selection algorithm found that it was amongst the features
 that best presented for defects within each data set. This figure counts how often a particular feature was selected across our 697 projects. 
 We call these features   ``locally important''
 since these results come from an analysis that is restricted to just one data set.


\begin{table}[!t]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\rowcolor[HTML]{C0C0C0} 
Attributes & Cluster 1 & Cluster 2 & Cluster 3 & Cluster 4 \\ \hline
NRev       & \bluecheck         &           &           &           \\ \hline
NFix       &           & \bluecheck         &           &           \\ \hline
LOC        & \bluecheck         & \bluecheck         &           &           \\ \hline
CC         & \bluecheck         &           &           &           \\ \hline
NIV        & \bluecheck         &           & \bluecheck         &           \\ \hline
NPM        & \bluecheck         &           & \bluecheck         &           \\ \hline
CBO        &           & \bluecheck         &           &           \\ \hline
FANOUT     &           &           & \bluecheck         &           \\ \hline
CL         &           &           & \bluecheck         & \bluecheck         \\ \hline
RCC        &           &           &           & \bluecheck         \\ \hline
NOC        &           &           & \bluecheck         & \bluecheck         \\ \hline
\end{tabular}
\caption{Important features in the four   GENERAL\_level1 models found by this study. 
Features are included here only if
their $\beta$ coefficients  are   not   ``close'' to zero (where ``close'' means $-0.1 \le \beta \le 0.1$).}
\label{tbl:coefs_4}
\end{table}

To make it easy to compare these locally and globally important features,  we color in \textcolor{red}{{\bf RED}}
all the bars of \fig{FSS_level1} that come from features
also found in  Table~\ref{tbl:coefs_4}. Note that only a small subset of the locally important
features are actually globally important.

Note also that, at first glance, there  seems to be some agreement on what is both locally and globally important. On the left-hand side of  \fig{FSS_level1}, we see that six of the top ten locally important features are colored red; i.e. they are also globally important. 

On the other hand, a more careful analysis shows things that it is not wise to simply assume that the most locally important features are usually important for most projects:
\bi
\item
Consider the top four most common locally important features: NFix, NRev, CL, RCC. These features appear in  25\%, 25\%, 50\%, 25\% 
(respectively) of the clusters shown in Table~\ref{tbl:coefs_4}. 
\item
Also, consider the height of the bars in \fig{FSS_level1}. Nearly all the locally important features are important in less than 25\% of projects.
\ei
That is,  it is a mistake to assume that something that is  important for a few local projects is globally useful to dozens to hundreds of projects. 
That said, the good news from this analysis is that in terms of defect prediction,
 (a)~hundreds of projects can be reasoned about using just four groups; (b)~within each group, there exist some general conclusions (see Table~\ref{tbl:coefs_4}). 
 
 



\section{Threats to Validity}
\label{sec:validity}

As with any large scale empirical study, biases can affect the final
results. Therefore, any conclusions made from this work
must be considered with the following issues in mind:

(a) \textit{Evaluation Bias}: 
In  {\bf  RQ3, RQ4} and, {\bf RQ5} we have shown the performance of local model, hierarchical bellwether models, default bellwether model and compared them using statistical tests on their performance to make conclusion about presence of generality in SE datasets. While those results are true, that conclusion is scoped by the evaluation metrics we used to write this paper. It is possible that using other measurements, there may well be a difference in these different kinds of projects. This is a matter that needs to be explored in future research.  

    
(b) \textit{Construct Validity}: At various places in this report, we made engineering decisions about (e.g.) choice of machine learning models, hierarchical clustering algorithm, selecting feature vectors for each project. While those decisions were made using advice from the literature, we acknowledge that other constructs might lead to different conclusions. 

(c) \textit{External Validity}: For this study, we have relied on data collected by Zhang et al.~\cite{zhang15} for their studies. The metrics collected for each project were done using a commercialized tool called ``Understand''. There is a possibility that calculation of metrics or labeling of defective vs non-defective using other tools or methods may result in different outcomes. That said, the ``Understand'' is a commercialized tool which has detailed documentation about the metrics calculations and Zhang et al. has shared their scripts and process to convert the metrics to a usable format and has described the approach to label defects.  

We have relied on issues marked as a `bug' or `enhancement' to count bugs or enhancements, and bug or enhancement resolution times. In Github, a bug or enhancement might not be marked in an issue but in commits. There is also a possibility that the team of that project might be using different tag identifiers for bugs and enhancements. To reduce the impact of this problem, we  did take precautionary step to (e.g.,) include various tag identifiers from Cabot et al.~\cite{cabot2015exploring}. We also took precaution to remove any pull merge requests from the commits to remove any extra contributions added to the hero programmer. 

(d) \textit{Statistical Validity}: To increase the validity of our results, we applied two statistical tests, bootstrap and the a12. Hence, anytime in this paper we reported that ``X was different from Y'' then that report was based on both an effect size and a statistical significance test.
 
(e) \textit{Sampling Bias}: Our conclusions are based on the 697 projects collected by Zhang et al.~\cite{zhang15} for their studies. It is possible that different initial projects would have lead to different conclusions. That said, this sample is very large so we have some confidence that this sample represents an interesting range of projects.  
 



\section{Conclusion}
\label{sec:concl}
In this paper, we have proposed a new transfer learning  bellwether method   called GENERAL.
While GENERAL only reflects on a small percent of the projects,
its hierarchical methods find projects which yield models
whose performance is comparable to anything else we studied
in this analysis.
 

One reason we recommend GENERAL is its scalabiity.
Pre-existing
bellwether methods are very slow. Here, we show
that a new method based on hierarchical reasoning
is both must faster (empirically) and can scale to much
larger sets of projects (theoretically). Such scalability
is vital to our research since, now that we have shown
we can reach general conclusions from 100s of projects,
our next goal is to analyze 1000s to 10,000s of projects.



Finally, we warn that much of the prior work on homogeneous transfer learning many  have complicated the homogeneous transfer learning process with needlessly
complicated methods.
  We strongly recommend that when building increasingly complex and expensive  methods, researchers should pause and compare their supposedly more sophisticated method against simpler alternatives. Going forward from this paper, we would recommend that the transfer learning community uses GENERAL as a baseline method against which they can test more complex methods.

% \section{Future Work}
% \label{sec:furute}



\section{Acknowledgments}
\label{sec:ack}

This work was partially funded by NSF Grant \#1908762.
 
\bibliographystyle{IEEEtran}
\bibliography{main} 
%  \newpage
 
\begin{minipage}{.9\linewidth}
\begin{IEEEbiography}[{\includegraphics[width=1in,clip,keepaspectratio]{figs/suvodeep.JPG}}]{Suvodeep Majumder}
 is a second year Ph.D. student in Computer Science at North Carolina State University.  
  His research interests include using large scale data mining and application of big data and artificial intelligence methods to solve problems in software engineering.
  \url{https://www.suvodeepmajumder.us}.
\end{IEEEbiography}
\begin{IEEEbiography}[{\includegraphics[width=1in,clip,keepaspectratio]{figs/krishna.jpg}}]{Rahul Krishna}, (Ph.D. 2019, North
Carolina State University) is a post-doc at Columbia University.
His research explores ways in which machine learning and AI can be used for software testing.
During his Ph.D. he worked on actionable analytics for software engineering to  develop algorithms that go beyond prediction to generate insights that can assist decision making.   \url{http://rkrsn.us}
\end{IEEEbiography}
\begin{IEEEbiography}[{\includegraphics[width=1in,clip,keepaspectratio]{figs/tim.png}}]{Tim Menzies} (IEEE Fellow)
is a Professor in CS at NcState  His research interests include software engineering (SE), data mining, artificial intelligence, search-based SE, and open access science. \url{http://menzies.us}
\end{IEEEbiography}
\end{minipage}

\newpage
\input{response.tex}

\end{document}

